{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code generation with an RNN\n",
    "Modified from https://www.tensorflow.org/tutorials/text/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.utils.get_file(\n",
    "    \"insults.txt\",\n",
    "    \"https://gist.githubusercontent.com/MythicManiac/dc9e1216105ff317b7dd14014896b8a4/raw/623f22945918f7b1ab66dab80fcff91a56184cf3/messages.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lucian push first lane fast pls\n",
      "so we get lvl 2\n",
      "i start on blue\n",
      "and kill them\n",
      "sebi da?\n",
      "lux no ward ? :S\n",
      "ok ill try\n",
      "jo\n",
      "you have 1\n",
      "was mit skxype los?\n",
      "lux supp itens\n",
      "-.-\n",
      "no supp\n",
      "ap\n",
      "supp ap\n",
      "nah\n",
      "just don't farm ok ?\n",
      "is sometnhin\n",
      "ap\n",
      "supp ap pls\n",
      "no farm\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "text = open(dataset, \"r\").read()\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 unique characters in dataset\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '\\\\', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~', '£', '¤', '¨', '¬', '´', '¹', 'º', '¼', 'Â', 'Ã', 'ã', '‚', '„', '€']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "print(f\"{len(vocabulary)} unique characters in dataset\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_to_index = {\n",
    "    character: index\n",
    "    for index, character\n",
    "    in enumerate(vocabulary)\n",
    "}\n",
    "index_to_character = np.array(vocabulary)\n",
    "\n",
    "vectorized_dataset = np.array([\n",
    "    character_to_index[character]\n",
    "    for character in text\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"\\n\": 0,\n",
      "    \" \": 1,\n",
      "    \"!\": 2,\n",
      "    \"\\\"\": 3,\n",
      "    \"#\": 4,\n",
      "    \"$\": 5,\n",
      "    \"%\": 6,\n",
      "    \"&\": 7,\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(character_to_index, indent=4)[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character to integer mapping example\n",
      "lucian push f\n",
      "[74 83 65 71 63 76  1 78 83 81 70  1 68]\n"
     ]
    }
   ],
   "source": [
    "print(\"Character to integer mapping example\")\n",
    "print(text[:13])\n",
    "print(vectorized_dataset[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 1222 examples per epoch\n"
     ]
    }
   ],
   "source": [
    "maximum_sequence_length = 30\n",
    "examples_per_epoch = len(text) // (maximum_sequence_length + 1)\n",
    "print(f\"Training with {examples_per_epoch} examples per epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l\n",
      "u\n",
      "c\n",
      "i\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "dataset_helper = tf.data.Dataset.from_tensor_slices(vectorized_dataset)\n",
    "for i in dataset_helper.take(5):\n",
    "    print(index_to_character[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'lucian push first lane fast pls'\n",
      "'\\nso we get lvl 2\\ni start on blu'\n",
      "'e\\nand kill them\\nsebi da?\\nlux no'\n",
      "' ward ? :S\\nok ill try\\njo\\nyou ha'\n",
      "'ve 1\\nwas mit skxype los?\\nlux su'\n"
     ]
    }
   ],
   "source": [
    "sequences = dataset_helper.batch(\n",
    "    maximum_sequence_length + 1,\n",
    "    drop_remainder=True\n",
    ")\n",
    "for item in sequences.take(5):\n",
    "    print(repr(\"\".join(index_to_character[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((30,), (30,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_data = sequence[:-1]\n",
    "    target_data = sequence[1:]\n",
    "    return input_data, target_data\n",
    "\n",
    "prepared_dataset = sequences.map(split_input_target)\n",
    "prepared_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: 'lucian push first lane fast pl'\n",
      "Target data: 'ucian push first lane fast pls'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in prepared_dataset.take(1):\n",
    "    print(f\"Input data:\", repr(\"\".join(index_to_character[input_example.numpy()])))\n",
    "    print(f\"Target data:\", repr(\"\".join(index_to_character[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 30), (64, 30)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_dataset = (\n",
    "    prepared_dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")\n",
    "\n",
    "shuffled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(vocabulary)\n",
    "# Tutorial had the embedding dimension at 256, but after looking up some\n",
    "# metrics and what it should be based on, I decided to drop it down to 64.\n",
    "# See https://en.wikipedia.org/wiki/Word2vec#Dimensionality\n",
    "# Also https://datascience.stackexchange.com/a/48194\n",
    "embedding_dimension = 64\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (64, None, 64)            6656      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (64, None, 1024)          3348480   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (64, None, 104)           106600    \n",
      "=================================================================\n",
      "Total params: 3,461,736\n",
      "Trainable params: 3,461,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"./training-checkpoints/lol-toxicity-generation-with-an-rnn\"\n",
    "def build_model(vocabulary_size, embedding_dimension, rnn_units, batch_size):\n",
    "    model =  tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            vocabulary_size,\n",
    "            embedding_dimension,\n",
    "            batch_input_shape=[batch_size, None]\n",
    "        ),\n",
    "        tf.keras.layers.GRU(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            stateful=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        ),\n",
    "        tf.keras.layers.Dense(vocabulary_size),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    embedding_dimension=embedding_dimension,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest:\n",
    "    model.load_weights(latest)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 30, 104) # (batch_size, sequence_length, vocabulary_size)\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in shuffled_dataset.take(1):\n",
    "    predictions = model(input_batch)\n",
    "    print(predictions.shape, \"# (batch_size, sequence_length, vocabulary_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 75,  47,   8,  81,  81,  15, 102,  53,  65,   3,  31,  95,  53,\n",
       "         8,  93,  64,  46,  59,  44, 102, 103,  44,  30,  11, 100,  24,\n",
       "        88,  17,  34,  81], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apparently random sampling should be used rather than argmax to avoid loops.\n",
    "# So this piece of code uses a the output value as a probability, rather\n",
    "# than just choosing the one that's highest.\n",
    "sampled_indices = tf.random.categorical(predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape (batch_size, sequence_length, vocabulary_size)\n",
      "(64, 30, 104) \n",
      "\n",
      "scalar_loss: 4.644359\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels,\n",
    "        logits,\n",
    "        from_logits=True,\n",
    "    )\n",
    "\n",
    "batch_loss = loss(target_batch, predictions)\n",
    "print(\"Predictions shape (batch_size, sequence_length, vocabulary_size)\")\n",
    "print(predictions.shape, \"\\n\")\n",
    "print(\"scalar_loss:\", batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.abspath(\n",
    "    os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    ")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 19 steps\n",
      "Epoch 1/90\n",
      "19/19 [==============================] - 3s 150ms/step - loss: 4.6519\n",
      "Epoch 2/90\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 3.4899\n",
      "Epoch 3/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 3.2394\n",
      "Epoch 4/90\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 3.1147\n",
      "Epoch 5/90\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 2.9623\n",
      "Epoch 6/90\n",
      "19/19 [==============================] - 1s 79ms/step - loss: 2.8458\n",
      "Epoch 7/90\n",
      "19/19 [==============================] - 1s 78ms/step - loss: 2.7563\n",
      "Epoch 8/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 2.6871\n",
      "Epoch 9/90\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 2.6382\n",
      "Epoch 10/90\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 2.5963\n",
      "Epoch 11/90\n",
      "19/19 [==============================] - 1s 76ms/step - loss: 2.5600\n",
      "Epoch 12/90\n",
      "19/19 [==============================] - 1s 75ms/step - loss: 2.5195\n",
      "Epoch 13/90\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 2.4855\n",
      "Epoch 14/90\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 2.4483\n",
      "Epoch 15/90\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 2.4063\n",
      "Epoch 16/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 2.3670\n",
      "Epoch 17/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 2.3272\n",
      "Epoch 18/90\n",
      "19/19 [==============================] - 1s 79ms/step - loss: 2.2828\n",
      "Epoch 19/90\n",
      "19/19 [==============================] - 1s 76ms/step - loss: 2.2404\n",
      "Epoch 20/90\n",
      "19/19 [==============================] - 1s 79ms/step - loss: 2.1959\n",
      "Epoch 21/90\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 2.1568\n",
      "Epoch 22/90\n",
      "19/19 [==============================] - 1s 78ms/step - loss: 2.1087\n",
      "Epoch 23/90\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 2.0678\n",
      "Epoch 24/90\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 2.0145\n",
      "Epoch 25/90\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 1.9672\n",
      "Epoch 26/90\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.9180\n",
      "Epoch 27/90\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 1.8671\n",
      "Epoch 28/90\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 1.8066\n",
      "Epoch 29/90\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 1.7448\n",
      "Epoch 30/90\n",
      "19/19 [==============================] - 2s 85ms/step - loss: 1.6832\n",
      "Epoch 31/90\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 1.6247\n",
      "Epoch 32/90\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 1.5565\n",
      "Epoch 33/90\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 1.4730\n",
      "Epoch 34/90\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 1.4040\n",
      "Epoch 35/90\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 1.3220\n",
      "Epoch 36/90\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 1.2423\n",
      "Epoch 37/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 1.1653\n",
      "Epoch 38/90\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 1.0850\n",
      "Epoch 39/90\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 1.0074\n",
      "Epoch 40/90\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 0.9407\n",
      "Epoch 41/90\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.8756\n",
      "Epoch 42/90\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 0.8137\n",
      "Epoch 43/90\n",
      "19/19 [==============================] - 2s 87ms/step - loss: 0.7628\n",
      "Epoch 44/90\n",
      "19/19 [==============================] - 2s 89ms/step - loss: 0.7183\n",
      "Epoch 45/90\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.6800\n",
      "Epoch 46/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.6467\n",
      "Epoch 47/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.6240\n",
      "Epoch 48/90\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 0.5916\n",
      "Epoch 49/90\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5759\n",
      "Epoch 50/90\n",
      "19/19 [==============================] - 1s 76ms/step - loss: 0.5589\n",
      "Epoch 51/90\n",
      "19/19 [==============================] - 1s 78ms/step - loss: 0.5405\n",
      "Epoch 52/90\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.5294\n",
      "Epoch 53/90\n",
      "19/19 [==============================] - 1s 76ms/step - loss: 0.5158\n",
      "Epoch 54/90\n",
      "19/19 [==============================] - 2s 88ms/step - loss: 0.5033\n",
      "Epoch 55/90\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 0.4883\n",
      "Epoch 56/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.4841\n",
      "Epoch 57/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.4769\n",
      "Epoch 58/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.4647\n",
      "Epoch 59/90\n",
      "19/19 [==============================] - 2s 80ms/step - loss: 0.4629\n",
      "Epoch 60/90\n",
      "19/19 [==============================] - 1s 79ms/step - loss: 0.4527\n",
      "Epoch 61/90\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 0.4457\n",
      "Epoch 62/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.4361\n",
      "Epoch 63/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.4288\n",
      "Epoch 64/90\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.4340\n",
      "Epoch 65/90\n",
      "19/19 [==============================] - 2s 80ms/step - loss: 0.4232\n",
      "Epoch 66/90\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.4223\n",
      "Epoch 67/90\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 0.4142\n",
      "Epoch 68/90\n",
      "19/19 [==============================] - 1s 78ms/step - loss: 0.4111\n",
      "Epoch 69/90\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.4053\n",
      "Epoch 70/90\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 0.4002\n",
      "Epoch 71/90\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.3954\n",
      "Epoch 72/90\n",
      "19/19 [==============================] - 2s 86ms/step - loss: 0.3978\n",
      "Epoch 73/90\n",
      "19/19 [==============================] - 2s 79ms/step - loss: 0.3907\n",
      "Epoch 74/90\n",
      "19/19 [==============================] - 2s 82ms/step - loss: 0.3839\n",
      "Epoch 75/90\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 0.3807\n",
      "Epoch 76/90\n",
      "19/19 [==============================] - 2s 83ms/step - loss: 0.3799\n",
      "Epoch 77/90\n",
      "19/19 [==============================] - 1s 79ms/step - loss: 0.3750\n",
      "Epoch 78/90\n",
      "19/19 [==============================] - 1s 78ms/step - loss: 0.3734\n",
      "Epoch 79/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.3753\n",
      "Epoch 80/90\n",
      "19/19 [==============================] - 1s 78ms/step - loss: 0.3681\n",
      "Epoch 81/90\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 0.3656\n",
      "Epoch 82/90\n",
      "19/19 [==============================] - 1s 77ms/step - loss: 0.3626\n",
      "Epoch 83/90\n",
      "19/19 [==============================] - 2s 79ms/step - loss: 0.3560\n",
      "Epoch 84/90\n",
      "19/19 [==============================] - 1s 79ms/step - loss: 0.3523\n",
      "Epoch 85/90\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.3536\n",
      "Epoch 86/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.3532\n",
      "Epoch 87/90\n",
      "19/19 [==============================] - 2s 81ms/step - loss: 0.3511\n",
      "Epoch 88/90\n",
      "19/19 [==============================] - 2s 84ms/step - loss: 0.3452\n",
      "Epoch 89/90\n",
      "19/19 [==============================] - 2s 80ms/step - loss: 0.3436\n",
      "Epoch 90/90\n",
      "19/19 [==============================] - 1s 78ms/step - loss: 0.3431\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    shuffled_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (1, None, 64)             6656      \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, None, 1024)           3348480   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 104)            106600    \n",
      "=================================================================\n",
      "Total params: 3,461,736\n",
      "Trainable params: 3,461,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocabulary_size,\n",
    "    embedding_dimension,\n",
    "    rnn_units,\n",
    "    batch_size=1,\n",
    ")\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    characters_to_generate = 1000\n",
    "    \n",
    "    input_eval = [\n",
    "        character_to_index[character]\n",
    "        for character in start_string\n",
    "    ]\n",
    "    # tf.expand_dims inserts a dimension at the specified index.\n",
    "    # In this case it converts our shape from (n,) to (1, n,)\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    generated_output = []\n",
    "    \n",
    "    temperature = 1.0\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(characters_to_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # tf.squeeze here does the opposite of tf.expand_dims\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        predictions /= temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)\n",
    "        predicted_id = predicted_id[-1, 0].numpy()\n",
    "        \n",
    "        # Pass in the predicted character as input on the next round\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        generated_output.append(index_to_character[predicted_id])\n",
    "    \n",
    "    return f\"{start_string}{''.join(generated_output)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banned that 2 vayne\n",
      "7vayne take red\n",
      "draven\n",
      "dont you noob\n",
      "we losing the tower and sthite\n",
      "next time you don'trck\n",
      "ide\n",
      "u suck,too\n",
      "wrtaa\n",
      "told u\n",
      "nah\n",
      "u dont even red dreed\n",
      "alp and --- win\n",
      "yes i then then push\n",
      "NS then uwer think i shot ult ?\n",
      "u suck,too\n",
      "what happend\n",
      "lol?\n",
      "well\n",
      "okai need\n",
      "hamadceam\n",
      "como veas. . . omg ma her\n",
      "Absolutly not\n",
      "fking riven dmg\n",
      "i'm comming top\n",
      "muchroo.\n",
      "play come and j4\n",
      "pla -.-\n",
      "why ult ?\n",
      "baiooob\n",
      "needs teemo veas. . .\n",
      "omg go nn and shen unt ge\n",
      "tryn xD\n",
      "heacand ge and stfu\n",
      "back\n",
      "relax en y us the lee sin\n",
      "qq piece of sheit e eluse\n",
      "lol\n",
      "xDDooooooo\n",
      "im only nk\n",
      "ranged\n",
      "he halich back for wards\n",
      "orianna\n",
      "is here tart\n",
      "gg\n",
      "thx\n",
      "me car the one rag nice team communication\n",
      "you're the one or not takign inhib\n",
      "reprot me alon\n",
      "n\n",
      "bat we are lori dives, they dive us\n",
      "other lanes cull tryndafed\n",
      "u suck,too\n",
      "wrtaa\n",
      "told u\n",
      "nah\n",
      "u dont kno\n",
      "om -.-\n",
      "non\n",
      "np\n",
      "sry\n",
      "cant lux too low and riven feed riven\n",
      "8 tryn\n",
      "and die 8 time\n",
      "REPORT EZAL ME WA work\n",
      "bl dont run\n",
      "OOO\n",
      "gj\n",
      "WH to work\n",
      "bot soon\n",
      "wp\n",
      "ty\n",
      "YIN got ot xtt's easy to play luc\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"banned \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
