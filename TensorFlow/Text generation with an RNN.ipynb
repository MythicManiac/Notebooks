{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation with an RNN\n",
    "https://www.tensorflow.org/tutorials/text/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.utils.get_file(\n",
    "    \"shakespeare.txt\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = open(dataset, \"r\").read()\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters in dataset\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "print(f\"{len(vocabulary)} unique characters in dataset\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_to_index = {\n",
    "    character: index\n",
    "    for index, character\n",
    "    in enumerate(vocabulary)\n",
    "}\n",
    "index_to_character = np.array(vocabulary)\n",
    "\n",
    "vectorized_dataset = np.array([\n",
    "    character_to_index[character]\n",
    "    for character in text\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"\\n\": 0,\n",
      "    \" \": 1,\n",
      "    \"!\": 2,\n",
      "    \"$\": 3,\n",
      "    \"&\": 4,\n",
      "    \"'\": 5,\n",
      "    \",\": 6,\n",
      "    \"-\": 7,\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(character_to_index, indent=4)[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character to integer mapping example\n",
      "First Citizen\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print(\"Character to integer mapping example\")\n",
    "print(text[:13])\n",
    "print(vectorized_dataset[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 11043 examples per epoch\n"
     ]
    }
   ],
   "source": [
    "maximum_sequence_length = 100\n",
    "examples_per_epoch = len(text) // (maximum_sequence_length + 1)\n",
    "print(f\"Training with {examples_per_epoch} examples per epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "dataset_helper = tf.data.Dataset.from_tensor_slices(vectorized_dataset)\n",
    "for i in dataset_helper.take(5):\n",
    "    print(index_to_character[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequences = dataset_helper.batch(\n",
    "    maximum_sequence_length + 1,\n",
    "    drop_remainder=True\n",
    ")\n",
    "for item in sequences.take(5):\n",
    "    print(repr(\"\".join(index_to_character[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((100,), (100,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_data = sequence[:-1]\n",
    "    target_data = sequence[1:]\n",
    "    return input_data, target_data\n",
    "\n",
    "prepared_dataset = sequences.map(split_input_target)\n",
    "prepared_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in prepared_dataset.take(1):\n",
    "    print(f\"Input data:\", repr(\"\".join(index_to_character[input_example.numpy()])))\n",
    "    print(f\"Target data:\", repr(\"\".join(index_to_character[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_dataset = (\n",
    "    prepared_dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")\n",
    "\n",
    "shuffled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(vocabulary)\n",
    "# Tutorial had the embedding dimension at 256, but after looking up some\n",
    "# metrics and what it should be based on, I decided to drop it down to 64.\n",
    "# See https://en.wikipedia.org/wiki/Word2vec#Dimensionality\n",
    "# Also https://datascience.stackexchange.com/a/48194\n",
    "embedding_dimension = 64\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (64, None, 64)            4160      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 1024)          3348480   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 3,419,265\n",
      "Trainable params: 3,419,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"./training-checkpoints/text-generation-with-an-rnn\"\n",
    "def build_model(vocabulary_size, embedding_dimension, rnn_units, batch_size):\n",
    "    model =  tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            vocabulary_size,\n",
    "            embedding_dimension,\n",
    "            batch_input_shape=[batch_size, None]\n",
    "        ),\n",
    "        tf.keras.layers.GRU(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            stateful=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        ),\n",
    "        tf.keras.layers.Dense(vocabulary_size),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    embedding_dimension=embedding_dimension,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocabulary_size)\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in shuffled_dataset.take(1):\n",
    "    predictions = model(input_batch)\n",
    "    print(predictions.shape, \"# (batch_size, sequence_length, vocabulary_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25, 21, 34, 20, 39, 30, 16,  1, 21, 21, 21, 10,  0, 22, 52,  1,  1,\n",
       "       61, 44,  1, 52, 53, 59,  1,  1, 43, 50, 60,  1, 57,  1, 50, 53, 55,\n",
       "       53, 51, 40, 43, 43, 58, 41, 43, 12, 39, 47, 43, 52, 45, 57, 47, 53,\n",
       "       59,  1,  1,  6, 50, 60,  0,  0, 32, 19, 33, 17, 17, 26,  1, 25, 24,\n",
       "       21, 38, 13, 14, 17, 32, 20, 10,  0, 26, 43, 58,  1, 39, 46, 39, 59,\n",
       "       45, 57, 53, 57, 57, 58,  1, 57, 43, 57, 50,  5, 46, 63,  1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apparently random sampling should be used rather than argmax to avoid loops.\n",
    "# So this piece of code uses a the output value as a probability, rather\n",
    "# than just choosing the one that's highest.\n",
    "sampled_indices = tf.random.categorical(predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " \" RICHARD III:\\nAy, if yourself's remembrance wrong yourself.\\n\\nQUEEN ELIZABETH:\\nBut thou didst kill my\" \n",
      "\n",
      "Output:\n",
      " \"MIVHaRD III:\\nJn  wf nou  elv s loqombeetce?aiengsiou  ,lv\\n\\nTGUEEN MLIZABETH:\\nNet ahaugsosst sesl'hy \"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", repr(\"\".join(index_to_character[input_batch[0]])), \"\\n\")\n",
    "print(\"Output:\\n\", repr(\"\".join(index_to_character[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape (batch_size, sequence_length, vocabulary_size)\n",
      "(64, 100, 65) \n",
      "\n",
      "scalar_loss: 1.310571\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels,\n",
    "        logits,\n",
    "        from_logits=True,\n",
    "    )\n",
    "\n",
    "batch_loss = loss(target_batch, predictions)\n",
    "print(\"Predictions shape (batch_size, sequence_length, vocabulary_size)\")\n",
    "print(predictions.shape, \"\\n\")\n",
    "print(\"scalar_loss:\", batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.abspath(\n",
    "    os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    ")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 172 steps\n",
      "Epoch 1/30\n",
      "172/172 [==============================] - 15s 85ms/step - loss: 1.2380\n",
      "Epoch 2/30\n",
      "172/172 [==============================] - 14s 80ms/step - loss: 1.2049\n",
      "Epoch 3/30\n",
      "172/172 [==============================] - 14s 81ms/step - loss: 1.1732\n",
      "Epoch 4/30\n",
      "172/172 [==============================] - 14s 80ms/step - loss: 1.1388\n",
      "Epoch 5/30\n",
      "172/172 [==============================] - 14s 81ms/step - loss: 1.1055\n",
      "Epoch 6/30\n",
      "172/172 [==============================] - 14s 81ms/step - loss: 1.0704\n",
      "Epoch 7/30\n",
      "172/172 [==============================] - 14s 80ms/step - loss: 1.0329\n",
      "Epoch 8/30\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 0.9964\n",
      "Epoch 9/30\n",
      "172/172 [==============================] - 14s 80ms/step - loss: 0.9614\n",
      "Epoch 10/30\n",
      "172/172 [==============================] - 14s 80ms/step - loss: 0.9247\n",
      "Epoch 11/30\n",
      "172/172 [==============================] - 14s 80ms/step - loss: 0.8928\n",
      "Epoch 12/30\n",
      "172/172 [==============================] - 14s 83ms/step - loss: 0.8601\n",
      "Epoch 13/30\n",
      "172/172 [==============================] - 14s 81ms/step - loss: 0.8311\n",
      "Epoch 14/30\n",
      "172/172 [==============================] - 14s 83ms/step - loss: 0.8064\n",
      "Epoch 15/30\n",
      "172/172 [==============================] - 14s 81ms/step - loss: 0.7821\n",
      "Epoch 16/30\n",
      "172/172 [==============================] - 14s 81ms/step - loss: 0.7606\n",
      "Epoch 17/30\n",
      "172/172 [==============================] - 14s 82ms/step - loss: 0.7411\n",
      "Epoch 18/30\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 0.7242\n",
      "Epoch 19/30\n",
      "172/172 [==============================] - 14s 80ms/step - loss: 0.7101\n",
      "Epoch 20/30\n",
      "172/172 [==============================] - 14s 80ms/step - loss: 0.6974\n",
      "Epoch 21/30\n",
      "172/172 [==============================] - 14s 82ms/step - loss: 0.6864\n",
      "Epoch 22/30\n",
      "172/172 [==============================] - 14s 80ms/step - loss: 0.6762\n",
      "Epoch 23/30\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 0.6691\n",
      "Epoch 24/30\n",
      "172/172 [==============================] - 14s 84ms/step - loss: 0.6603\n",
      "Epoch 25/30\n",
      "172/172 [==============================] - 14s 80ms/step - loss: 0.6509\n",
      "Epoch 26/30\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 0.6457\n",
      "Epoch 27/30\n",
      "172/172 [==============================] - 14s 81ms/step - loss: 0.6419\n",
      "Epoch 28/30\n",
      "172/172 [==============================] - 14s 80ms/step - loss: 0.6339\n",
      "Epoch 29/30\n",
      "172/172 [==============================] - 14s 80ms/step - loss: 0.6325\n",
      "Epoch 30/30\n",
      "172/172 [==============================] - 14s 79ms/step - loss: 0.6274\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    shuffled_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 64)             4160      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (1, None, 1024)           3348480   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 3,419,265\n",
      "Trainable params: 3,419,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocabulary_size,\n",
    "    embedding_dimension,\n",
    "    rnn_units,\n",
    "    batch_size=1,\n",
    ")\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    characters_to_generate = 1000\n",
    "    \n",
    "    input_eval = [\n",
    "        character_to_index[character]\n",
    "        for character in start_string\n",
    "    ]\n",
    "    # tf.expand_dims inserts a dimension at the specified index.\n",
    "    # In this case it converts our shape from (n,) to (1, n,)\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    generated_output = []\n",
    "    \n",
    "    temperature = 1.0\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(characters_to_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # tf.squeeze here does the opposite of tf.expand_dims\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        predictions /= temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)\n",
    "        predicted_id = predicted_id[-1, 0].numpy()\n",
    "        \n",
    "        # Pass in the predicted character as input on the next round\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        generated_output.append(index_to_character[predicted_id])\n",
    "    \n",
    "    return f\"{start_string}{''.join(generated_output)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Catulban\n",
      "And prince is call'd friendship: 'tis the fapalioor opes\n",
      "And plague in mighty fenLy my service or speed\n",
      "Whether for partly to rest to-night,\n",
      "Intending two swore to be part.\n",
      "\n",
      "KING RICHARD II:\n",
      "Thanks, good Lord deliver, you much.\n",
      "\n",
      "SAMPSON:\n",
      "Let us like a great day will hunt this wolk and leave and happy prove\n",
      "A serve the garments name my fortunes to we know your royal rest!\n",
      "And yet I come in; but old dangers.\n",
      "Would not they seek again: if any be, if he calls?\n",
      "\n",
      "First Soldier:\n",
      "Nor I, boys.\n",
      "\n",
      "Second Keeper:\n",
      "Help, nerd-applauded in thy veins,\n",
      "That companious villain in thy waychings I left to us,\n",
      "And every day to cure this feat, or dies\n",
      "Give signifies the have with him, proud attession\n",
      "To the present de you have dark declined;\n",
      "And high A man of worship,\n",
      "When, fairs a fortunation?\n",
      "\n",
      "FLORIZEL:\n",
      "We were fit\n",
      "To the poor king's demands will out of such\n",
      "d was here.\n",
      "You swear to then?\n",
      "\n",
      "ANGELO:\n",
      "He that hath been with the other credal order out, Romeo may die but fast By me, I, being aged gaunt:\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"ROMEO: \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
