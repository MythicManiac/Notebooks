{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code generation with an RNN\n",
    "Modified from https://www.tensorflow.org/tutorials/text/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKU_PREFIX = \"ID#\"\n",
      "# -- encoding: UTF-8 --\n",
      "import logging\n",
      "import sys\n",
      "\n",
      "from django.core.management import BaseCommand\n",
      "from django.db.transaction import atomic\n",
      "\n",
      "from apps.id_integration.product_importer import ShoopIDProductJsonImporter\n",
      "\n",
      "LOG = logging.\n"
     ]
    }
   ],
   "source": [
    "text = open(\"./data/legacy.py\", \"rb\").read().decode(\"utf-8\")\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 unique characters in dataset\n",
      "['\\t', '\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '©', '³', '½', 'Ä', 'Å', 'Ö', 'á', 'ä', 'æ', 'ï', 'ó', 'ö', 'ø', 'ú', 'ý', '̈', 'ω', 'Ṕ', '–', '’', '“', '”', '€', '⅔', '\\ue1c0']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = sorted(set(text))\n",
    "print(f\"{len(vocabulary)} unique characters in dataset\")\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_to_index = {\n",
    "    character: index\n",
    "    for index, character\n",
    "    in enumerate(vocabulary)\n",
    "}\n",
    "index_to_character = np.array(vocabulary)\n",
    "\n",
    "vectorized_dataset = np.array([\n",
    "    character_to_index[character]\n",
    "    for character in text\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"\\t\": 0,\n",
      "    \"\\n\": 1,\n",
      "    \" \": 2,\n",
      "    \"!\": 3,\n",
      "    \"\\\"\": 4,\n",
      "    \"#\": 5,\n",
      "    \"$\": 6,\n",
      "    \"%\": 7,...\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(character_to_index, indent=4)[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character to integer mapping example\n",
      "SKU_PREFIX = \n",
      "[53 45 55 65 50 52 39 40 43 58  2 31  2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Character to integer mapping example\")\n",
    "print(text[:13])\n",
    "print(vectorized_dataset[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 35117 examples per epoch\n"
     ]
    }
   ],
   "source": [
    "maximum_sequence_length = 100\n",
    "examples_per_epoch = len(text) // (maximum_sequence_length + 1)\n",
    "print(f\"Training with {examples_per_epoch} examples per epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "K\n",
      "U\n",
      "_\n",
      "P\n"
     ]
    }
   ],
   "source": [
    "dataset_helper = tf.data.Dataset.from_tensor_slices(vectorized_dataset)\n",
    "for i in dataset_helper.take(5):\n",
    "    print(index_to_character[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'SKU_PREFIX = \"ID#\"\\n# -- encoding: UTF-8 --\\nimport logging\\nimport sys\\n\\nfrom django.core.management imp'\n",
      "'ort BaseCommand\\nfrom django.db.transaction import atomic\\n\\nfrom apps.id_integration.product_importer i'\n",
      "'mport ShoopIDProductJsonImporter\\n\\nLOG = logging.getLogger()\\nLOG.setLevel(logging.INFO)\\n\\nSTDOUT_HANDLE'\n",
      "'R = logging.StreamHandler(sys.stdout)\\nSTDOUT_HANDLER.setLevel(logging.INFO)\\nLOG.addHandler(STDOUT_HAN'\n",
      "'DLER)\\n\\n\\nclass Command(BaseCommand):\\n    \"\"\"\\n    Import ID product data from the JSON format they prov'\n"
     ]
    }
   ],
   "source": [
    "sequences = dataset_helper.batch(\n",
    "    maximum_sequence_length + 1,\n",
    "    drop_remainder=True\n",
    ")\n",
    "for item in sequences.take(5):\n",
    "    print(repr(\"\".join(index_to_character[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((100,), (100,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_data = sequence[:-1]\n",
    "    target_data = sequence[1:]\n",
    "    return input_data, target_data\n",
    "\n",
    "prepared_dataset = sequences.map(split_input_target)\n",
    "prepared_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data: 'SKU_PREFIX = \"ID#\"\\n# -- encoding: UTF-8 --\\nimport logging\\nimport sys\\n\\nfrom django.core.management im'\n",
      "Target data: 'KU_PREFIX = \"ID#\"\\n# -- encoding: UTF-8 --\\nimport logging\\nimport sys\\n\\nfrom django.core.management imp'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in prepared_dataset.take(1):\n",
    "    print(f\"Input data:\", repr(\"\".join(index_to_character[input_example.numpy()])))\n",
    "    print(f\"Target data:\", repr(\"\".join(index_to_character[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "shuffled_dataset = (\n",
    "    prepared_dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")\n",
    "\n",
    "shuffled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(vocabulary)\n",
    "# Tutorial had the embedding dimension at 256, but after looking up some\n",
    "# metrics and what it should be based on, I decided to drop it down to 64.\n",
    "# See https://en.wikipedia.org/wiki/Word2vec#Dimensionality\n",
    "# Also https://datascience.stackexchange.com/a/48194\n",
    "embedding_dimension = 64\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 64)            7808      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3348480   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (64, None, 512)           2362368   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 122)           62586     \n",
      "=================================================================\n",
      "Total params: 5,781,242\n",
      "Trainable params: 5,781,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"./training-checkpoints/code-generation-with-an-rnn-2\"\n",
    "def build_model(vocabulary_size, embedding_dimension, rnn_units, batch_size):\n",
    "    model =  tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(\n",
    "            vocabulary_size,\n",
    "            embedding_dimension,\n",
    "            batch_input_shape=[batch_size, None]\n",
    "        ),\n",
    "        tf.keras.layers.GRU(\n",
    "            rnn_units,\n",
    "            return_sequences=True,\n",
    "            stateful=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        ),\n",
    "        tf.keras.layers.GRU(\n",
    "            rnn_units // 2,\n",
    "            return_sequences=True,\n",
    "            stateful=True,\n",
    "            recurrent_initializer=\"glorot_uniform\",\n",
    "        ),\n",
    "        tf.keras.layers.Dense(vocabulary_size),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    vocabulary_size=vocabulary_size,\n",
    "    embedding_dimension=embedding_dimension,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest:\n",
    "    model.load_weights(latest)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 122) # (batch_size, sequence_length, vocabulary_size)\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in shuffled_dataset.take(1):\n",
    "    predictions = model(input_batch)\n",
    "    print(predictions.shape, \"# (batch_size, sequence_length, vocabulary_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([87, 77, 85, 91, 69, 82, 85, 18,  2,  2,  2,  2, 85, 69,  2,  2,  2,\n",
       "       71, 81, 84, 65, 73, 71, 14, 85, 31,  2, 93, 87, 70, 71, 80, 31, 82,\n",
       "       73, 14, 56, 84, 81, 73, 87, 69, 86, 52, 80, 70, 71, 84, 85, 28, 45,\n",
       "       47, 35, 71, 38, 54, 41, 35, 40, 80, 54, 39, 80, 54, 39, 54, 39, 48,\n",
       "       40, 38, 47, 39, 54, 49,  2,  2,  2,  2,  2,  1,  2,  2,  2,  2, 84,\n",
       "       71, 86, 81, 78, 85, 31, 31,  2,  2, 81, 87, 84, 84, 91, 80],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apparently random sampling should be used rather than argmax to avoid loops.\n",
    "# So this piece of code uses a the output value as a probability, rather\n",
    "# than just choosing the one that's highest.\n",
    "sampled_indices = tf.random.categorical(predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " 'querier(\\n        shop=shop, ordering=ProductOrdering.LEAST_EXPENSIVE_FIRST\\n    )\\n    results = queri' \n",
      "\n",
      "Output:\n",
      " 'uksycps0    sc   eor_ge,s= {uden=pg,VroguctRnders:KMAeDTGAFnTEnTETENFDMETO     \\n    retols==  ourryn'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", repr(\"\".join(index_to_character[input_batch[0]])), \"\\n\")\n",
    "print(\"Output:\\n\", repr(\"\".join(index_to_character[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape (batch_size, sequence_length, vocabulary_size)\n",
      "(64, 100, 122) \n",
      "\n",
      "scalar_loss: 1.9175811\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels,\n",
    "        logits,\n",
    "        from_logits=True,\n",
    "    )\n",
    "\n",
    "batch_loss = loss(target_batch, predictions)\n",
    "print(\"Predictions shape (batch_size, sequence_length, vocabulary_size)\")\n",
    "print(predictions.shape, \"\\n\")\n",
    "print(\"scalar_loss:\", batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.tf.scheduler import SGDRScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.abspath(\n",
    "    os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    ")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    ")\n",
    "\n",
    "learningrate_callback = SGDRScheduler(\n",
    "    min_lr=1e-5,\n",
    "    max_lr=1e-2,\n",
    "    steps_per_epoch=548,\n",
    "    lr_decay=0.9,\n",
    "    cycle_length=5,\n",
    "    mult_factor=1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 548 steps\n",
      "Epoch 1/30\n",
      "548/548 [==============================] - 60s 109ms/step - loss: 1.3349\n",
      "Epoch 2/30\n",
      "548/548 [==============================] - 61s 111ms/step - loss: 1.3167\n",
      "Epoch 3/30\n",
      "548/548 [==============================] - 61s 111ms/step - loss: 1.2850\n",
      "Epoch 4/30\n",
      "548/548 [==============================] - 61s 111ms/step - loss: 1.2601\n",
      "Epoch 5/30\n",
      "548/548 [==============================] - 60s 109ms/step - loss: 1.2400\n",
      "Epoch 6/30\n",
      "548/548 [==============================] - 60s 110ms/step - loss: 1.2203\n",
      "Epoch 7/30\n",
      "548/548 [==============================] - 60s 109ms/step - loss: 1.1996\n",
      "Epoch 8/30\n",
      "548/548 [==============================] - 60s 109ms/step - loss: 1.1796\n",
      "Epoch 9/30\n",
      "548/548 [==============================] - 60s 109ms/step - loss: 1.1638\n",
      "Epoch 10/30\n",
      "548/548 [==============================] - 62s 114ms/step - loss: 1.1510\n",
      "Epoch 11/30\n",
      "548/548 [==============================] - 63s 115ms/step - loss: 1.1413\n",
      "Epoch 12/30\n",
      "548/548 [==============================] - 63s 115ms/step - loss: 1.1357\n",
      "Epoch 13/30\n",
      "548/548 [==============================] - 61s 111ms/step - loss: 1.1331\n",
      "Epoch 14/30\n",
      "548/548 [==============================] - 60s 110ms/step - loss: 1.1319\n",
      "Epoch 15/30\n",
      "548/548 [==============================] - 61s 112ms/step - loss: 1.1324\n",
      "Epoch 16/30\n",
      "548/548 [==============================] - 60s 109ms/step - loss: 1.1342\n",
      "Epoch 17/30\n",
      "548/548 [==============================] - 60s 110ms/step - loss: 1.1376\n",
      "Epoch 18/30\n",
      "548/548 [==============================] - 62s 113ms/step - loss: 1.1433\n",
      "Epoch 19/30\n",
      "548/548 [==============================] - 59s 108ms/step - loss: 1.1564\n",
      "Epoch 20/30\n",
      "548/548 [==============================] - 60s 109ms/step - loss: 1.1631\n",
      "Epoch 21/30\n",
      "548/548 [==============================] - 60s 109ms/step - loss: 1.1821\n",
      "Epoch 22/30\n",
      "548/548 [==============================] - 58s 107ms/step - loss: 1.1945\n",
      "Epoch 23/30\n",
      "548/548 [==============================] - 58s 105ms/step - loss: 1.2161\n",
      "Epoch 24/30\n",
      "548/548 [==============================] - 60s 109ms/step - loss: 1.2432\n",
      "Epoch 25/30\n",
      "548/548 [==============================] - 60s 110ms/step - loss: 1.2693\n",
      "Epoch 26/30\n",
      "548/548 [==============================] - 62s 112ms/step - loss: 1.2973\n",
      "Epoch 27/30\n",
      "548/548 [==============================] - 61s 111ms/step - loss: 1.3348\n",
      "Epoch 28/30\n",
      "548/548 [==============================] - 61s 112ms/step - loss: 1.4010\n",
      "Epoch 29/30\n",
      "548/548 [==============================] - 60s 110ms/step - loss: 1.5639\n",
      "Epoch 30/30\n",
      "548/548 [==============================] - 59s 107ms/step - loss: 1.4868\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    shuffled_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback, learningrate_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 64)             7808      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (1, None, 1024)           3348480   \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, None, 512)            2362368   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 122)            62586     \n",
      "=================================================================\n",
      "Total params: 5,781,242\n",
      "Trainable params: 5,781,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint = \"./training-checkpoints/mastermark-code-generation-with-an-rnn\\ckpt_29\"\n",
    "checkpoint = \"./training-checkpoints/mastermark-code-generation-with-an-rnn\\ckpt_139\"\n",
    "model = build_model(\n",
    "    vocabulary_size,\n",
    "    embedding_dimension,\n",
    "    rnn_units,\n",
    "    batch_size=1,\n",
    ")\n",
    "model.load_weights(checkpoint)\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    characters_to_generate = 1900\n",
    "    \n",
    "    input_eval = [\n",
    "        character_to_index[character]\n",
    "        for character in start_string\n",
    "    ]\n",
    "    # tf.expand_dims inserts a dimension at the specified index.\n",
    "    # In this case it converts our shape from (n,) to (1, n,)\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    generated_output = []\n",
    "    \n",
    "    temperature = 1.0\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(characters_to_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # tf.squeeze here does the opposite of tf.expand_dims\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        \n",
    "        predictions /= temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)\n",
    "        predicted_id = predicted_id[-1, 0].numpy()\n",
    "        \n",
    "        # Pass in the predicted character as input on the next round\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        generated_output.append(index_to_character[predicted_id])\n",
    "    \n",
    "    return f\"{start_string}{''.join(generated_output)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import User\n",
      "from shoop.identifiers to paramethod.\n",
      "\n",
      "   :type no: L{Element}\n",
      "        \"\"\"\n",
      "\n",
      "        if (\n",
      "            \"boxes\",\n",
      "            OrderSettingSelect,\n",
      "                    \"quantizer\": self.ids_weight),\n",
      "            \"tax_class\": {\"styles__in=f.number\n",
      "            str(text)\n",
      "                Product.ode mutatus and the product array,\n",
      "                colors=None,\n",
      "            product=product,\n",
      "        request,\n",
      "                _pa\"full_process_choices\",\n",
      "    ),\n",
      "    tax_pee:\n",
      "        # price internal installs class last in current tuple.\n",
      "        # no vilidity/LogEntryKind.\n",
      "        number\n",
      "       \n",
      "    :return: Item child address.integrap in PATTR or None)\n",
      "            return self.get_init__(self, key) from load_byys\n",
      "    gzeeding_price = formset_key(order)\n",
      "            except CopTion:\n",
      "                       mm_shop.org.uabaClasses = AdError(\"TESTED_REPLY_CACCE\n",
      "        ),\n",
      "    )\n",
      "        current_user.password = request.POST[0]\n",
      "    p = self.taxless_price\n",
      "        if package_response:\n",
      "            return False\n",
      "            themes  VARIATED = (130, _(u\"Tuulin, (user, \"fo\"), lategories=category)\n",
      "        1\n",
      "            .reference\n",
      "# along `description` rationaukser this\n",
      "dNE[\"]],\n",
      "    \"CHECE mail.join(\n",
      "            (prods, name, prb.find_emailed_by_find_form_data_id)\n",
      "\n",
      "    def waises(self):\n",
      "        return self.cleaned_data.get(\"Name\")\n",
      "        else:\n",
      "            len(model.objects.all_visible()\n",
      "            email = None\n",
      "\n",
      "            if not content:\n",
      "            self._campaign_model._initem_weight()\n",
      "        results = len(url)\n",
      "           bishmpor.SettingsLtPD)\n",
      "froms:\n",
      "        raise Problem(\n",
      "            \"Max_managers\",\n",
      "        )\n",
      "                    .order_by(\"general\")[:200.]\n",
      "            if value in os.path.join(payment_identifier, permission):\n",
      "        raise ValueError(\n",
      "           \"attribute.id\",\n",
      "    )\n",
      "        context[\"report_class\"]:\n",
      "            if not is not None\n",
      "        if \"?\"Content-DocumentID>\" % values[\"max_l\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"import \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
